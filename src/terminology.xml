    <!-- ===================================================== 2 -->
    <section anchor="terminology" title="Terminology">
      <!--  copy:          <t hangText=""></t>                 -->

      <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL",
      "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY",
      and "OPTIONAL" in this document are to be interpreted as
      described in 
      <xref target="RFC2119" />.</t>

      <t>Note that terms containing underscores (rather than
      spaces) appear in equations in the modeling sections. In some
      cases both forms are used for aesthetic reasons, they do not
      have different meanings.</t>

      <t>General Terminology:</t>

      <t>
        <list style="hanging">
          <t hangText="Target:">A general term for any parameter
          specified by or derived from the user's application or
          transport performance requirements.</t>
          <t hangText="Target Transport Performance:">Application
          or transport performance target values for the complete path. For
          Bulk Transport Capacity defined in this note the Target
          Transport Performance includes the Target Data Rate,
          Target RTT and Target MTU as described below.</t>
          <t hangText='Target Data Rate:'>The specified application
          data rate required for an application's proper operation.
          Conventional Bulk Transport Capacity (BTC) metrics are focused on the Target Data
          Rate, however these metrics had little or no predictive
          value because they do not consider the effects of the
          other two parameters of the Target Transport Performance,
          the RTT and MTU of the complete paths.</t>
          <t hangText='Target RTT (Round Trip Time):'>The specified
          baseline (minimum) RTT of the longest complete path over
          which the user expects to be able to meet the target
          performance. TCP and other transport protocol's ability
          to compensate for path problems is generally proportional
          to the number of round trips per second. The Target RTT
          determines both key parameters of the traffic patterns
          (e.g. burst sizes) and the thresholds on acceptable IP
          packet transfer statistics. The Target RTT must be
          specified considering appropriate packets sizes: MTU
          sized packets on the forward path, ACK sized packets
          (typically header_overhead) on the return path. Note that
          Target RTT is specified and not measured, MBM measurements
	  derived for a given target_RTT will be applicable to
	  any path with a smaller RTTs.</t>
          <t hangText='Target MTU (Maximum Transmission Unit):'>The
          specified maximum MTU supported by the complete path the
          over which the application expects to meet the target
          performance. Assume a 1500 Byte MTU when testing unless otherwise
          specified. If some subpath has a smaller MTU, then it
          becomes the Target MTU for the complete path, and all
          model calculations and subpath tests must use the same
          smaller MTU.</t>
          <t hangText="Targeted IP Diagnostic Suite (TIDS):">A set
          of IP diagnostic tests designed to determine if an
          otherwise ideal complete path containing the subpath
          under test can sustain flows at a specific
          target_data_rate using target_MTU sized packets when the
          RTT of the complete path is target_RTT.</t>
          <t hangText="Fully Specified Targeted IP Diagnostic Suite (FS-TIDS):">
          A TIDS together with additional specification such as
          "type-p" <xref target="RFC2330" />, etc. which are out of scope for this document,
          but need to be drawn from other standards documents.</t>
          <t hangText="Bulk Transport Capacity:">Bulk Transport
          Capacity Metrics evaluate an Internet path's ability to
          carry bulk data, such as large files, streaming (non-real
          time) video, and under some conditions, web images and
          other content. Prior efforts to define BTC metrics have
          been based on 
          <xref target="RFC3148" />, which predates our
          understanding of TCP and the requirements described in 
          <xref target="background" /></t>
          <t hangText="IP diagnostic tests:">Measurements or
          diagnostics to determine if packet transfer statistics
          meet some precomputed target.</t>
          <t hangText="traffic patterns:">The temporal patterns or
          burstiness of traffic generated by applications over
          transport protocols such as TCP. There are several
          mechanisms that cause bursts at various time scales as
          described in 
          <xref target="tcp" />. Our goal here is to mimic the
          range of common patterns (burst sizes and rates, etc),
          without tying our applicability to specific applications,
          implementations or technologies, which are sure to become
          stale.</t>
          <t hangText="packet transfer statistics:">Raw, detailed
          or summary statistics about packet transfer properties of
          the IP layer including packet losses, ECN Congestion
          Experienced (CE) marks, reordering, or any other
          properties that may be germane to transport
          performance.</t>
          <t hangText="packet loss ratio:">As defined in 
          <xref target="RFC7680" />.</t>
          <t hangText="apportioned:">To divide and allocate, for
          example budgeting packet loss across multiple subpaths
          such that the losses will accumulate to less than a
          specified end-to-end loss ratio. Apportioning metrics is
          essentially the inverse of the process described in 
          <xref target="RFC5835" />.</t>
          <t hangText="open loop:">A control theory term used to
          describe a class of techniques where systems that
          naturally exhibit circular dependencies can be analyzed
          by suppressing some of the dependencies, such that the
          resulting dependency graph is acyclic.</t>
          <!--  copy:
<t hangText="@">@ @</t>
-->
        </list>
      </t>

      <t>Terminology about paths, etc. See 
      <xref target="RFC2330" /> and 
      <xref target="RFC7398" /> for existing terms and definitions.</t>

      <t>
        <list style="hanging">
          <t hangText="data sender:">Host sending data and
          receiving ACKs.</t>
          <t hangText="data receiver:">Host receiving data and
          sending ACKs.</t>
          <t hangText="complete path:">The end-to-end path from the
          data sender to the data receiver.</t>
          <t hangText="subpath:">A portion of the complete path.
          Note that there is no requirement that subpaths be
          non-overlapping. A subpath can be a small as a single
          device, link or interface.</t>
          <t hangText="measurement point:">Measurement points as
          described in 
          <xref target="RFC7398" />.</t>
          <t hangText="test path:">A path between two measurement
          points that includes a subpath of the complete path under
          test. If the measurement points are off path, the test
          path may include "test leads" between the measurement
          points and the subpath.</t>
          <t hangText="dominant bottleneck:">The bottleneck that
          generally determines most of packet transfer statistics
          for the entire path. It typically determines a flow's
          self clock timing, packet loss and ECN Congestion
          Experienced (CE) marking rate, with other potential
          bottlenecks having less effect on the packet transfer
          statistics. See 
          <xref target="tcp" /> on TCP properties.</t>
          <t hangText="front path:">The subpath from the data
          sender to the dominant bottleneck.</t>
          <t hangText="back path:">The subpath from the dominant
          bottleneck to the receiver.</t>
          <t hangText="return path:">The path taken by the ACKs
          from the data receiver to the data sender.</t>
          <t hangText="cross traffic:">Other, potentially
          interfering, traffic competing for network resources
          (bandwidth and/or queue capacity).</t>
        </list>
      </t>

      <t>Properties determined by the complete path and
      application. These are described in more detail in 
      <xref target="target" />.</t>

      <t>
        <list style="hanging">
          <t hangText="Application Data Rate:">General term for the
          data rate as seen by the application above the transport
          layer in bytes per second. This is the payload data rate,
          and explicitly excludes transport and lower level headers
          (TCP/IP or other protocols), retransmissions and other
          overhead that is not part to the total quantity of data
          delivered to the application.</t>
          <t hangText="IP rate:">The actual number of IP-layer
          bytes delivered through a subpath, per unit time,
          including TCP and IP headers, retransmits and other
          TCP/IP overhead. Follows from IP-type-P Link Usage 
          <xref target="RFC5136" />.</t>
          <t hangText="IP capacity:">The maximum number of IP-layer
          bytes that can be transmitted through a subpath, per unit
          time, including TCP and IP headers, retransmits and other
          TCP/IP overhead. Follows from IP-type-P Link Capacity 
          <xref target="RFC5136" />.</t>
          <t hangText="bottleneck IP capacity:">The IP capacity of
          the dominant bottleneck in the forward path. All
          throughput maximizing protocols estimate this capacity by
          observing the IP rate delivered through the bottleneck.
          Most protocols derive their self clocks from the timing
          of this data. See 
          <xref target="tcp" /> and 
          <xref target="complex" /> for more details.</t>
          <t hangText="implied bottleneck IP capacity:">This is the
          bottleneck IP capacity implied by the ACKs returning from
          the receiver. It is determined by looking at how much
          application data the ACK stream at the sender reports
          delivered to the data receiver per unit time at various
          time scales. If the return path is thinning, batching or
          otherwise altering the ACK timing the implied bottleneck
          IP capacity over short time scales might be substantially
          larger than the bottleneck IP capacity averaged over a
          full RTT. Since TCP derives its clock from the data
          delivered through the bottleneck, the front path must have
          sufficient buffering to absorb any data bursts at the
          dimensions (size and IP rate) implied by the ACK
          stream, which are potentially doubled during slowstart. If the
          return path is not altering the ACK stream, then the
          implied bottleneck IP capacity will be the same as the
          bottleneck IP capacity. See 
          <xref target="tcp" /> and 
          <xref target="complex" /> for more details.</t>
          <t hangText="sender interface rate:">The IP rate which
          corresponds to the IP capacity of the data sender's
          interface. Due to sender efficiency algorithms including
          technologies such as TCP segmentation offload (TSO),
          nearly all modern servers deliver data in bursts at full
          interface link rate. Today 1 or 10 Gb/s are typical.</t>
          <t hangText="Header_overhead:">The IP and TCP header
          sizes, which are the portion of each MTU not available
          for carrying application payload. Without loss of
          generality this is assumed to be the size for returning
          acknowledgments (ACKs). For TCP, the Maximum Segment
          Size (MSS) is the Target MTU minus the
          header_overhead.</t>
        </list>
      </t>

      <t>Basic parameters common to models and subpath tests are
      defined here are described in more detail in 
      <xref target="models" />. Note that these are mixed between
      application transport performance (excludes headers) and IP
      performance (which include TCP headers and retransmissions as
      part of the IP payload).</t>

      <t>
        <list style="hanging">
          <t hangText="Window [size]:">The total quantity of data
          carried by packets in-flight 
          plus the data represented by ACKs circulating in the
          network is referred to as the window. See 
          <xref target="tcp" />. Sometimes used with other
          qualifiers (congestion window, cwnd or receiver window)
          to indicate which mechanism is controlling the
          window.</t>
          <t hangText="pipe size:">A general term for number of
          packets needed in flight (the window size) to exactly
          fill some network path or subpath. It corresponds to the
          window size which maximizes network power, the observed
          data rate divided by the observed RTT. Often used with
          additional qualifiers to specify which path, or under
          what conditions, etc.</t>
          <t hangText="target_window_size:">The average number of
          packets in flight (the window size) needed to meet the
          Target Data Rate, for the specified Target RTT, and MTU.
          It implies the scale of the bursts that the network might
          experience.</t>
          <t hangText="run length:">A general term for the
          observed, measured, or specified number of packets that
          are (expected to be) delivered between losses or ECN
          Congestion Experienced (CE) marks. Nominally one over the
          sum of the loss and ECN CE marking probabilities, if
          there are independently and identically distributed.</t>
          <t hangText="target_run_length:">The target_run_length is
          an estimate of the minimum number of non-congestion
          marked packets needed between losses or ECN Congestion
          Experienced (CE) marks necessary to attain the
          target_data_rate over a path with the specified
          target_RTT and target_MTU, as computed by a mathematical
          model of TCP congestion control. A reference calculation
          is shown in 
          <xref target="models" /> and alternatives in 
          <xref target="derive" /></t>
          <t hangText="reference target_run_length:">
          target_run_length computed precisely by the method in 
          <xref target="models" />. This is likely to be
          slightly more conservative than required by modern TCP
          implementations.</t>
        </list>
      </t>

      <t>Ancillary parameters used for some tests:</t>

      <t>
        <list style="hanging">
          <t hangText="derating:">Under some conditions the
          standard models are too conservative. The modeling
          framework permits some latitude in relaxing or "derating"
          some test parameters as described in 
          <xref target="derate" /> in exchange for a more stringent
          TIDS validation procedures, described in 
          <xref target="validation" />.
	  Models can be derated by including a multiplicative
	  derating factor to make tests less stringent.
	  </t>
          <t hangText="subpath_IP_capacity:">The IP capacity of a
          specific subpath.
	  </t>
          <t hangText="test path:">A subpath of a complete path
          under test.</t>
          <t hangText="test_path_RTT:">The RTT observed between two
          measurement points using packet sizes that are consistent
          with the transport protocol.
	  This is generally MTU sized packets
          of the forward path, header_overhead sized packets on the
          return path.</t>
          <t hangText="test_path_pipe:">The pipe size of a test
          path. Nominally the test path RTT times the test path
          IP_capacity.</t>
          <t hangText="test_window:">The smallest window sufficient
	  to meet or exceed
          the target_rate when operating with a pure self clock over a test path.
	  The test_window is typically given by
          ceiling(target_data_rate*test_path_RTT/(target_MTU-header_overhead))
	  but see the discussion in <xref target="complex" /> about
	  the effects of channel scheduling on RTT.
	  On some test paths the test_window may need to be
	  adjusted slightly to compensate for the RTT being
	  inflated by the devices that schedule packets.</t>
          <!-- <t hangText="@:">@ @</t> -->
        </list>
      </t>

      <t>The terminology below is used to define temporal patterns
      for test stream. These patterns are designed to mimic TCP
      behavior, as described in 
      <xref target="tcp" />. 
      <list style="hanging">
        <t hangText="packet headway:">Time interval between
        packets, specified from the start of one to the start of
        the next. e.g. If packets are sent with a 1 mS headway,
        there will be exactly 1000 packets per second.</t>
        <t hangText="burst headway:">Time interval between bursts,
        specified from the start of the first packet one burst to
        the start of the first packet of the next burst. e.g. If 4
        packet bursts are sent with a 1 mS burst headway, there
        will be exactly 4000 packets per second.</t>
        <t hangText="paced single packets:">Send individual packets
        at the specified rate or packet headway.</t>
        <t hangText="paced bursts:">Send bursts on a timer. Specify
        any 3 of: average data rate, packet size, burst size
        (number of packets) and burst headway (burst start to
        start). By default the bursts are assumed to occur at full sender
        interface rate, such that the packet headway within each
        burst is the minimum supported by the sender's interface.
        Under some conditions it is useful to explicitly specify
        the packet headway within each burst.</t>
        <t hangText="slowstart rate:">Mimic TCP slowstart by
        sending 4 packet paced bursts at an average data rate equal
        to twice the implied bottleneck IP capacity (but not more
        than the sender interface rate). This is a two level burst
        pattern described in more detail in 
        <xref target="paced" />. If the implied bottleneck IP
        capacity is more than half of the sender interface rate,
        slowstart rate becomes sender interface rate.</t>
        <t hangText="slowstart burst:">Mimic one round of TCP
        slowstart by sending a specified number of packets packets
        in a two level burst pattern that resembles slowstart.</t>
        <t hangText="repeated slowstart bursts:">Repeat Slowstart
        bursts once per target_RTT. For TCP each burst would be
        twice as large as the prior burst, and the sequence would
        end at the first ECN CE mark or lost packet. For
        measurement, all slowstart bursts would be the same size
        (nominally target_window_size but other sizes might be
        specified), and the ECN CE marks and lost packets are
        counted.</t>
      </list></t>

      <t>The tests described in this note can be grouped according
      to their applicability.</t>

      <t>
        <list style="hanging">
          <t hangText="Capacity tests:">Capacity tests determine if
          a network subpath has sufficient capacity to deliver the
          Target Transport Performance. As long as the test stream
          is within the proper envelope for the Target Transport
          Performance, the average packet losses or ECN Congestion
          Experienced (CE) marks must be below the statistical criteria
          computed by the model. As such, capacity tests reflect
          parameters that can transition from passing to failing as
          a consequence of cross traffic, additional presented load
          or the actions of other network users. By definition,
          capacity tests also consume significant network resources
          (data capacity and/or queue buffer space), and the test
          schedules must be balanced by their cost.</t>
          <t hangText="Monitoring tests:">Monitoring tests are
          designed to capture the most important aspects of a
          capacity test, but without presenting excessive ongoing
          load themselves. As such they may miss some details of
          the network's performance, but can serve as a useful
          reduced-cost proxy for a capacity test, for example to
          support continuous production network monitoring.</t>
          <t hangText="Engineering tests:">Engineering tests
          evaluate how network algorithms (such as AQM and channel
          allocation) interact with TCP-style self clocked
          protocols and adaptive congestion control based on packet
          loss and ECN Congestion Experienced (CE) marks. These
          tests are likely to have complicated interactions with
          cross traffic and under some conditions can be inversely
          sensitive to load. For example a test to verify that an
          AQM algorithm causes ECN CE marks or packet drops early
          enough to limit queue occupancy may experience a false
          pass result in the presence of cross traffic. It is
          important that engineering tests be performed under a
          wide range of conditions, including both in situ and
          bench testing, and over a wide variety of load
          conditions. Ongoing monitoring is less likely to be
          useful for engineering tests, although sparse in situ
          testing might be appropriate.</t>
        </list>
      </t>
    </section>
