    <!-- ======================================================== 9 -->
    <section anchor="examples" title="An Example">

      <t>In this section a we illustrate a TIDS designed to confirm
      that an access ISP can reliably deliver HD video from
      multiple content providers to all of their customers. With
      modern codecs, minimal HD video (720p) generally fits in 2.5
      Mb/s. Due to their geographical size, network topology and
      modem characteristics the ISP determines that most content is within
      a 50 mS RTT of their users (This example RTT is a sufficient to cover
      the propagation delay to continental Europe or either US coast
      with low delay modems or somewhat smaller geographical regions if the modems
      require additional delay to implement advanced compression and
      error recovery).
      </t>

      <texttable anchor="HDvideo_table">
        <preamble>2.5 Mb/s over a 50 ms path</preamble>
        <ttcol>End-to-End Parameter</ttcol>
        <ttcol>value</ttcol>
        <ttcol>units</ttcol>
        <c>target_rate</c>
        <c>2.5</c>
        <c>Mb/s</c>
        <c>target_RTT</c>
        <c>50</c>
        <c>ms</c>
        <c>target_MTU</c>
        <c>1500</c>
        <c>bytes</c>
        <c>header_overhead</c>
        <c>64</c>
        <c>bytes</c>
        <c></c>
        <c></c>
        <c></c>
        <c>target_window_size</c>
        <c>11</c>
        <c>packets</c>
        <c>target_run_length</c>
        <c>363</c>
        <c>packets</c>
      </texttable>

      <t>Table 1 shows the default TCP model with no derating, and
      as such is quite conservative. The simplest TIDS would be to
      use the sustained burst test, described in 
      <xref target="sustained_burst" />. Such a test would send 11
      packet bursts every 50mS, and confirming that there was no
      more than 1 packet loss per 33 bursts (363 total packets in
      1.650 seconds).</t>

      <t>Since this number represents is the entire end-to-end loss
      budget, independent subpath tests could be implemented by
      apportioning the packet loss ratio across subpaths. For
      example 50% of the losses might be allocated to the access or
      last mile link to the user, 40% to the interconnects with
      other ISPs and 1% to each internal hop (assuming no more than
      10 internal hops). Then all of the subpaths can be tested
      independently, and the spatial composition of passing
      subpaths would be expected to be within the end-to-end loss
      budget.
      </t>

      <!-- @@@@ move elsewhere or new doc? -->
      <section anchor="applicability" title="Observations about applicability">

	<t>Guidance on deploying and using MBM belong in a
	future document.  However this example illustrates some
	the issues that may need to be considered.</t>

      <t>Note that a another ISP, with different geographical
      coverage, topology or modem technology may need to assume
      a different target_RTT, and as a consequence different
      target_window_size and target_run_length, even for the
      same target_data rate.
      One of the implications of this is that infrastructure
      shared by multiple ISPs, such as inter-exchange points (IXPs) and
      other interconnects may need to be evaluated on the basis
      of the most stringent target_window_size and
      target_run_length of any participating ISP.
      One way to do this might be to choose target parameters for
      evaluating such shared infrastructure on the basis of a
      hypothetical reference path that does not necessarily
      match any actual paths.
      </t>

      <t>Testing interconnects has generally been problematic:
      conventional performance tests run between measurement points
      adjacent to either side of the interconnect are not
      generally useful. Unconstrained TCP tests, such as iPerf 
      <xref target="iPerf" /> are usually overly aggressive due
      to the small RTT (often less than 1 mS). With a short RTT
      these tools are likely to report inflated data rates because on a
      short RTT these tools can tolerate very high packet loss
      ratios and can push other cross traffic off of the network.
      As a consequence these measurements are useless for predicting actual user
      performance over longer paths, and may themselves be quite
      disruptive. Model
      Based Metrics solves this problem. The interconnect can be
      evaluated with the same TIDS as other subpaths.
      Continuing our example, if the interconnect is
      apportioned 40% of the losses, 11
      packet bursts sent every 50mS should have fewer than one loss
      per 82 bursts (902 packets).</t>

      </section>

      <!-- <t> @@ SPRT example here, or remove reference above </t> -->
      <!--   @@ consider 10 or 20 mS RTT. -->
      <!--
<section anchor="SDvideo" title="Far serving SD streaming video">

<t>Standard Quality video typically fits in 1 Mb/s [@@SDvideo].   This can be reasonably delivered via longer paths with larger.   We assume 100mS.</t>


<texttable anchor="SDvideo_table">
<preamble>1 Mb/s over a 100 ms path</preamble>
<ttcol>End-to-End Parameter </ttcol>
<ttcol>Value</ttcol>
<ttcol>units</ttcol>
<c>target_rate</c><c>1</c><c>Mb/s</c>
<c>target_RTT</c><c>100</c><c>ms</c>
<c>traget_MTU</c><c>1500</c><c>bytes</c>


<c>target_window_size</c><c>9</c><c>packets</c>
<c>target_run_length</c><c>243</c><c>packets</c>
</texttable>

<t>This example uses the most conservative TCP model and no derating.</t>


</section>
<section anchor="BulkData" title="Bulk delivery of remote scientific data">

<t>This example corresponds to 100 Mb/s bulk scientific data over a moderately long RTT.  Note that the target_run_length is infeasible for most networks.</t>


<texttable anchor="Bulk
Data_table">
<preamble>100 Mb/s over a 200 ms path</preamble>
<ttcol>End-to-End Parameter </ttcol>
<ttcol>Value</ttcol>
<ttcol>units</ttcol>
<c>target_rate</c><c>100</c><c>Mb/s</c>
<c>target_RTT</c><c>200</c><c>ms</c>
<c>traget_MTU</c><c>1500</c><c>bytes</c>


<c>target_window_size</c><c>1741</c><c>packets</c>
<c>target_run_length</c><c>9093243</c><c>packets</c>
</texttable>








</section>
-->
    </section>
