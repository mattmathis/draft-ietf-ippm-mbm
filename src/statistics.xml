      <section anchor="statistics"
      title="Statistical criteria for estimating run_length">

        <t>When evaluating the observed run_length, we need to
        determine appropriate packet stream sizes and acceptable
        error levels for efficient measurement. In practice, can we
        compare the empirically estimated packet loss and ECN
        Congestion Experienced (CE) marking ratios with the targets
        as the sample size grows? How large a sample is needed to
        say that the measurements of packet transfer indicate a
        particular run length is present?</t>

        <t>The generalized measurement can be described as
        recursive testing: send packets (individually or in
        patterns) and observe the packet transfer performance
        (packet loss ratio or other metric, any marking we
        define).</t>

        <t>As each packet is sent and measured, we have an ongoing
        estimate of the performance in terms of the ratio of packet
        loss or ECN CE mark to total packets (i.e. an empirical
        probability). We continue to send until conditions support
        a conclusion or a maximum sending limit has been
        reached.</t>

        <t>We have a target_mark_probability, 1 mark per
        target_run_length, where a "mark" is defined as a lost
        packet, a packet with ECN CE mark, or other signal. This
        constitutes the null Hypothesis:</t>

        <t>
          <list style="hanging">
            <t hangText="H0:">no more than one mark in
            target_run_length = 3*(target_window_size)^2
            packets</t>
          </list>
        </t>

        <t>and we can stop sending packets if on-going measurements
        support accepting H0 with the specified Type I error =
        alpha (= 0.05 for example).</t>

        <t>We also have an alternative Hypothesis to evaluate: if
        performance is significantly lower than the
        target_mark_probability. Based on analysis of typical
        values and practical limits on measurement duration, we
        choose four times the H0 probability:</t>

        <t>
          <list style="hanging">
            <t hangText="H1:">one or more marks in
            (target_run_length/4) packets</t>
          </list>
        </t>

        <t>and we can stop sending packets if measurements support
        rejecting H0 with the specified Type II error = beta (=
        0.05 for example), thus preferring the alternate hypothesis
        H1.</t>

        <t>H0 and H1 constitute the Success and Failure outcomes
        described elsewhere in the memo, and while the ongoing
        measurements do not support either hypothesis the current
        status of measurements is inconclusive.</t>

        <t>The problem above is formulated to match the Sequential
        Probability Ratio Test (SPRT) 
	<xref target="Wald45" /> and 
        <xref target="Montgomery90" />. Note that as originally framed
        the events under consideration were all manufacturing
        defects. In networking, ECN CE marks and lost packets are
        not defects but signals, indicating that the transport
        protocol should slow down.</t>

        <t>The Sequential Probability Ratio Test also starts with a
        pair of hypothesis specified as above:</t>

        <t>
        <list style="hanging">
          <t hangText="H0:">p0 = one defect in
          target_run_length</t>
          <t hangText="H1:">p1 = one defect in
          target_run_length/4</t>
        </list>As packets are sent and measurements collected, the
        tester evaluates the cumulative defect count against two
        boundaries representing H0 Acceptance or Rejection (and
        acceptance of H1):</t>

        <t>
        <list style="hanging">
          <t hangText="Acceptance line:">Xa = -h1 + s*n</t>
          <t hangText="Rejection line:">Xr = h2 + s*n</t>
        </list>where n increases linearly for each packet sent
        and</t>

        <t>
        <list style="hanging">
          <t hangText="h1 =">{ log((1-alpha)/beta) }/k</t>
          <t hangText="h2 =">{ log((1-beta)/alpha) }/k</t>
          <t hangText="k  =">log{ (p1(1-p0)) / (p0(1-p1)) }</t>
          <t hangText="s  =">[ log{ (1-p0)/(1-p1) } ]/k</t>
        </list>for p0 and p1 as defined in the null and alternative
        Hypotheses statements above, and alpha and beta as the Type
        I and Type II errors.</t>

        <t>The SPRT specifies simple stopping rules:</t>

        <t>
          <list style="symbols">

            <t>Xa &lt; defect_count(n) &lt; Xr: continue
            testing</t>

            <t>defect_count(n) &lt;= Xa: Accept H0</t>

            <t>defect_count(n) &gt;= Xr: Accept H1</t>
          </list>
        </t>

        <t>The calculations above are implemented in the R-tool for
        Statistical Analysis 
        <xref target="Rtool" /> , in the add-on package for
        Cross-Validation via Sequential Testing (CVST) 
        <xref target="CVST" />.</t>

        <t>Using the equations above, we can calculate the minimum
        number of packets (n) needed to accept H0 when x defects
        are observed. For example, when x = 0:</t>

        <t>
          <list style="hanging">
            <t hangText="Xa = 0">= -h1 + s*n</t>
            <t hangText="and">n = h1 / s</t>
          </list>
        </t>

	<t>
	  Note that the derivations in <xref target="Wald45" /> and
	  <xref target="Montgomery90" /> differ.  Montgomery's
	  simplified derivation of SPRT may assume a Bernoulli processes,
	  where the packet loss probabilities are independent and
	  identically distributed, making the SPRT more
	  accessible.  Wald's seminal paper showed that this
	  assumption is not necessary.  It helps to remember that
	  the goal of SPRT is not to estimate the value of the packet
	  loss rate, but only whether or not the packet loss ratio is
	  likely low enough (when we accept the H0 null hypothesis)
	  yielding success; or too high (when we accept the H1
	  alternate hypothesis) yielding failure.
	</t>

      </section>
